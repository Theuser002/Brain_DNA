{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../../../src')\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import pyreadr\n",
    "import config\n",
    "import Dataset\n",
    "import time\n",
    "import train_MLP\n",
    "import argparse\n",
    "import utils\n",
    "import joblib\n",
    "\n",
    "from Dataset import CNS\n",
    "from torch.utils.data import DataLoader\n",
    "from Model import DNAMLP\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from torch.optim import Adam\n",
    "from utils import make_ndarray_from_csv, get_int_label, brier_score_tensor\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score \n",
    "from torch.nn.functional import softmax, one_hot\n",
    "from sklearn.model_selection import train_test_split\n",
    "from imblearn.over_sampling import SMOTE, RandomOverSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(epoch, model, train_loader, criterion, optimizer, device):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    total_loss = 0\n",
    "    total_bs = 0\n",
    "    model.to(device)\n",
    "    model.train()\n",
    "    \n",
    "    # For loop through all batches\n",
    "    all_labels = []\n",
    "    all_logits = []\n",
    "    for features, labels in train_loader:\n",
    "        # Move tensors to device\n",
    "        features = features.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        # Zero out gradient\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        logits = model(features)\n",
    "        # print(f'DJIWOEDJEWOD: {logits} - {labels}')\n",
    "        # print(f'HDUHOAWDHEOE: {logits.shape} - {labels.shape}')\n",
    "        loss = criterion(logits, labels)\n",
    "        # print(logits.shape, labels.shape)\n",
    "        \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Evaluation\n",
    "        total_loss += loss.item()\n",
    "        _, predicted = logits.max(1)\n",
    "        correct += predicted.eq(labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "        \n",
    "        # batch BS\n",
    "        batch_bs = brier_score_tensor(logits, labels)\n",
    "        total_bs += batch_bs\n",
    "        \n",
    "        # save logits and labels to calculate AUC\n",
    "        for logit, label in zip(logits, labels):\n",
    "            all_labels.append(label.item())\n",
    "            all_logits.append(np.array(logit.detach().cpu().numpy()))\n",
    "        \n",
    "    # epoch's avrage LL\n",
    "    train_loss = total_loss / len(train_loader)\n",
    "    # epoch's average acc & ME\n",
    "    train_acc = (correct / total) * 100.\n",
    "    train_me = 100 - train_acc\n",
    "    # epoch's average BS\n",
    "    train_bs = total_bs/len(train_loader)\n",
    "    # epoch's average AUC\n",
    "    # all_labels_one_hot = one_hot(torch.Tensor(np.array(all_labels)).long())\n",
    "    all_probs = softmax(torch.Tensor(np.array(all_logits)), dim = 1)\n",
    "    # train_auc = roc_auc_score(all_labels_one_hot, all_probs)\n",
    "    \n",
    "    all_preds = [np.argmax(prob) for prob in all_probs]\n",
    "    train_auc = roc_auc_score(all_labels, all_preds)\n",
    "    train_precision = precision_score(all_labels, all_preds)\n",
    "    train_recall = recall_score(all_labels, all_preds)\n",
    "    \n",
    "    return train_loss, train_acc, train_me, train_bs, train_auc, train_precision, train_recall\n",
    "\n",
    "def val_epoch(epoch, model, val_loader, criterion, device):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    total_loss = 0\n",
    "    total_bs = 0\n",
    "    model.to(device)\n",
    "    # For loop through all batches\n",
    "    with torch.no_grad():\n",
    "        # For loop through all batches\n",
    "        all_labels = []\n",
    "        all_logits = []\n",
    "        for features, labels in val_loader:\n",
    "            # Move tensors to device\n",
    "            features, labels = features.to(device), labels.to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            logits = model(features)\n",
    "            \n",
    "            # Evaluation and batch loss\n",
    "            loss = criterion(logits, labels)\n",
    "            total_loss += loss.item()\n",
    "            _, predicted = logits.max(1)\n",
    "            correct += predicted.eq(labels).sum().item()\n",
    "            total  += labels.size(0)\n",
    "            \n",
    "            # batch BS\n",
    "            batch_bs = brier_score_tensor(logits, labels)\n",
    "            total_bs += batch_bs\n",
    "            \n",
    "            # save logits and labels to calculate AUC\n",
    "            for logit, label in zip(logits,labels):\n",
    "                all_labels.append(label.item())\n",
    "                all_logits.append(np.array(logit.detach().cpu().numpy()))\n",
    "        \n",
    "        # epoch's average LL\n",
    "        val_loss = total_loss / len(val_loader)\n",
    "        # epoch's average acc & ME\n",
    "        val_acc = (correct / total) * 100\n",
    "        val_me = (100 - val_acc)\n",
    "        # epoch's average BS\n",
    "        val_bs = total_bs/len(val_loader)\n",
    "        # epoch's AUC\n",
    "        # all_labels_one_hot = one_hot(torch.Tensor(np.array(all_labels)).long())\n",
    "        all_probs = softmax(torch.Tensor(np.array(all_logits)), dim = 1)\n",
    "        # val_auc = roc_auc_score(all_labels_one_hot, all_probs)\n",
    "        \n",
    "        all_preds = [np.argmax(prob) for prob in all_probs]\n",
    "        val_auc = roc_auc_score(all_labels, all_preds)\n",
    "        val_precision = precision_score(all_labels, all_preds)\n",
    "        val_recall = recall_score(all_labels, all_preds)\n",
    "         \n",
    "    return val_loss, val_acc, val_me, val_bs, val_auc, val_precision, val_recall\n",
    "\n",
    "def run(class_name, fold, train_loader, val_loader, model, criterion, optimizer, config, save):\n",
    "    history = {'train_accs': [], 'train_losses': [], 'val_accs': [], 'val_losses': []}\n",
    "    \n",
    "    model.to(config['device'])\n",
    "    n_epochs = config['mlp_n_epochs']\n",
    "    BEST_STATES_DIR= config['MLP_BEST_STATES_DIR']\n",
    "    BEST_STATE_PATH = os.path.join(BEST_STATES_DIR, class_name, f'{fold}.pth')\n",
    "    diff_threshold = config['mlp_diff_threshold']\n",
    "    max_patience = config['mlp_max_patience']\n",
    "    patience = 0\n",
    "    \n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "        \n",
    "        train_loss, train_acc, train_me, train_bs, train_auc, train_precision, train_recall = train_epoch(epoch, model, train_loader, criterion, optimizer, config['device'])\n",
    "        val_loss, val_acc, val_me, val_bs, val_auc, val_precision, val_recall = val_epoch(epoch, model, val_loader, criterion, config['device'])\n",
    "        \n",
    "        history['train_accs'].append(train_acc)\n",
    "        history['train_losses'].append(train_loss)\n",
    "        history['val_accs'].append(val_acc)\n",
    "        history['val_losses'].append(val_loss)\n",
    "        \n",
    "        print(f'{class_name.upper()}] - {fold} - {epoch}/{n_epochs}')\n",
    "        print('train_loss: %.5f | train_acc: %.3f | train_precision: %.3f | train_recall: %.3f | train_auc: %.3f' % (train_loss, train_acc, train_precision, train_bs, train_auc))\n",
    "        print('val_loss: %.5f | val_acc: %.3f | val_precision: %.3f | val_recall: %.3f | val_auc: %.3f' % (val_loss, val_acc, val_precision, val_recall, val_auc))\n",
    "        \n",
    "        eval_file = open(config['MLP_EVALUATION_RESULTS'], 'a+')\n",
    "        eval_file.write(\n",
    "            f'\\n>>>>>>[{class_name.upper()}] - {fold} - {epoch}/{n_epochs}: train_loss: %.5f | train_acc: %.3f | train_precision: %.3f | train_recall: %.3f | train_auc: %.3f\\nval_loss: %.5f | val_acc: %.3f | val_precision: %.3f | val_recall: %.3f | val_auc: %.3f\\n' % (train_loss, train_acc, train_precision, train_recall, train_auc, val_loss, val_acc, val_precision, val_recall, val_auc)\n",
    "        )\n",
    "        eval_file.close()\n",
    "            \n",
    "        if val_loss == min(history['val_losses']):\n",
    "            if save.lower() == 'save':\n",
    "                print('Lowest validation loss => saving model weights...')\n",
    "                torch.save(model.state_dict(), BEST_STATE_PATH)\n",
    "        if len(history['val_losses']) > 1:\n",
    "            if abs(history['val_losses'][-2] - val_loss) < diff_threshold or history['val_losses'][-2] < val_loss:\n",
    "                patience = patience + 1\n",
    "                print(f'Patience increased to {patience}')\n",
    "                if patience == max_patience:\n",
    "                    print('Early stopping.')\n",
    "                    break\n",
    "            else:\n",
    "                patience = 0\n",
    "        print('---------------------------------------------')\n",
    "    return max(history['val_accs'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running mlp classifiers\n",
      "root: /media/data/hungnt/work/SourceCode/Brain_DNA/src/..\n",
      "device: cuda\n",
      "save mode: no_save\n",
      "Nerve - 5.1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((127, 10000), (127,))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Running mlp classifiers\")\n",
    "clf_cfg = config.classifier_config\n",
    "save = 'no_save'\n",
    "\n",
    "print(f'root: {config.root_dir}')\n",
    "print(f\"device: {clf_cfg['device']}\")\n",
    "print(f'save mode: {save}')\n",
    "\n",
    "folds = utils.inner_folds\n",
    "groups = utils.positive_groups\n",
    "\n",
    "fold = folds[np.random.randint(len(folds))]\n",
    "group = groups[np.random.randint(len(groups))]\n",
    "print(f'{group} - {fold}')\n",
    "\n",
    "# Read from csv to dataframe\n",
    "features, labels = make_ndarray_from_csv(group, fold, mode = 'all')\n",
    "features.shape, labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((63, 10000), (64, 10000))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_features, val_features, train_labels, val_labels = train_test_split(features, labels, test_size=0.5, random_state=42)\n",
    "train_features.shape, val_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 10000)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "smote = SMOTE(sampling_strategy = \"auto\", random_state = 42, k_neighbors = 4)\n",
    "new_train_features, new_train_labels = smote.fit_resample(train_features, train_labels)\n",
    "new_train_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running in no_save mode\n",
      "NERVE] - 5.1 - 1/30\n",
      "train_loss: 0.56632 | train_acc: 76.000 | train_precision: 0.760 | train_recall: 0.379 | train_auc: 0.760\n",
      "val_loss: 0.32810 | val_acc: 98.438 | val_precision: 0.981 | val_recall: 1.000 | val_auc: 0.962\n",
      "---------------------------------------------\n",
      "NERVE] - 5.1 - 2/30\n",
      "train_loss: 0.35567 | train_acc: 91.000 | train_precision: 0.956 | train_recall: 0.202 | train_auc: 0.910\n",
      "val_loss: 0.36141 | val_acc: 82.812 | val_precision: 1.000 | val_recall: 0.784 | val_auc: 0.892\n",
      "Patience increased to 1\n",
      "---------------------------------------------\n",
      "NERVE] - 5.1 - 3/30\n",
      "train_loss: 0.24475 | train_acc: 96.000 | train_precision: 1.000 | train_recall: 0.114 | train_auc: 0.960\n",
      "val_loss: 0.19425 | val_acc: 100.000 | val_precision: 1.000 | val_recall: 1.000 | val_auc: 1.000\n",
      "---------------------------------------------\n",
      "NERVE] - 5.1 - 4/30\n",
      "train_loss: 0.19256 | train_acc: 99.000 | train_precision: 1.000 | train_recall: 0.081 | train_auc: 0.990\n",
      "val_loss: 0.20208 | val_acc: 100.000 | val_precision: 1.000 | val_recall: 1.000 | val_auc: 1.000\n",
      "Patience increased to 1\n",
      "---------------------------------------------\n",
      "NERVE] - 5.1 - 5/30\n",
      "train_loss: 0.15197 | train_acc: 100.000 | train_precision: 1.000 | train_recall: 0.055 | train_auc: 1.000\n",
      "val_loss: 0.14585 | val_acc: 100.000 | val_precision: 1.000 | val_recall: 1.000 | val_auc: 1.000\n",
      "---------------------------------------------\n",
      "NERVE] - 5.1 - 6/30\n",
      "train_loss: 0.12730 | train_acc: 100.000 | train_precision: 1.000 | train_recall: 0.045 | train_auc: 1.000\n",
      "val_loss: 0.11462 | val_acc: 100.000 | val_precision: 1.000 | val_recall: 1.000 | val_auc: 1.000\n",
      "---------------------------------------------\n",
      "NERVE] - 5.1 - 7/30\n",
      "train_loss: 0.10737 | train_acc: 100.000 | train_precision: 1.000 | train_recall: 0.031 | train_auc: 1.000\n",
      "val_loss: 0.13091 | val_acc: 100.000 | val_precision: 1.000 | val_recall: 1.000 | val_auc: 1.000\n",
      "Patience increased to 1\n",
      "---------------------------------------------\n",
      "NERVE] - 5.1 - 8/30\n",
      "train_loss: 0.09081 | train_acc: 100.000 | train_precision: 1.000 | train_recall: 0.024 | train_auc: 1.000\n",
      "val_loss: 0.09852 | val_acc: 100.000 | val_precision: 1.000 | val_recall: 1.000 | val_auc: 1.000\n",
      "---------------------------------------------\n",
      "NERVE] - 5.1 - 9/30\n",
      "train_loss: 0.07608 | train_acc: 100.000 | train_precision: 1.000 | train_recall: 0.017 | train_auc: 1.000\n",
      "val_loss: 0.10178 | val_acc: 100.000 | val_precision: 1.000 | val_recall: 1.000 | val_auc: 1.000\n",
      "Patience increased to 1\n",
      "---------------------------------------------\n",
      "NERVE] - 5.1 - 10/30\n",
      "train_loss: 0.06532 | train_acc: 100.000 | train_precision: 1.000 | train_recall: 0.014 | train_auc: 1.000\n",
      "val_loss: 0.07947 | val_acc: 100.000 | val_precision: 1.000 | val_recall: 1.000 | val_auc: 1.000\n",
      "---------------------------------------------\n",
      "NERVE] - 5.1 - 11/30\n",
      "train_loss: 0.05959 | train_acc: 100.000 | train_precision: 1.000 | train_recall: 0.012 | train_auc: 1.000\n",
      "val_loss: 0.06921 | val_acc: 100.000 | val_precision: 1.000 | val_recall: 1.000 | val_auc: 1.000\n",
      "---------------------------------------------\n",
      "NERVE] - 5.1 - 12/30\n",
      "train_loss: 0.05385 | train_acc: 100.000 | train_precision: 1.000 | train_recall: 0.010 | train_auc: 1.000\n",
      "val_loss: 0.08368 | val_acc: 100.000 | val_precision: 1.000 | val_recall: 1.000 | val_auc: 1.000\n",
      "Patience increased to 1\n",
      "---------------------------------------------\n",
      "NERVE] - 5.1 - 13/30\n",
      "train_loss: 0.04857 | train_acc: 100.000 | train_precision: 1.000 | train_recall: 0.008 | train_auc: 1.000\n",
      "val_loss: 0.06304 | val_acc: 100.000 | val_precision: 1.000 | val_recall: 1.000 | val_auc: 1.000\n",
      "---------------------------------------------\n",
      "NERVE] - 5.1 - 14/30\n",
      "train_loss: 0.04487 | train_acc: 100.000 | train_precision: 1.000 | train_recall: 0.007 | train_auc: 1.000\n",
      "val_loss: 0.06300 | val_acc: 100.000 | val_precision: 1.000 | val_recall: 1.000 | val_auc: 1.000\n",
      "Patience increased to 1\n",
      "---------------------------------------------\n",
      "NERVE] - 5.1 - 15/30\n",
      "train_loss: 0.04073 | train_acc: 100.000 | train_precision: 1.000 | train_recall: 0.007 | train_auc: 1.000\n",
      "val_loss: 0.05848 | val_acc: 100.000 | val_precision: 1.000 | val_recall: 1.000 | val_auc: 1.000\n",
      "---------------------------------------------\n",
      "NERVE] - 5.1 - 16/30\n",
      "train_loss: 0.03785 | train_acc: 100.000 | train_precision: 1.000 | train_recall: 0.005 | train_auc: 1.000\n",
      "val_loss: 0.05049 | val_acc: 100.000 | val_precision: 1.000 | val_recall: 1.000 | val_auc: 1.000\n",
      "---------------------------------------------\n",
      "NERVE] - 5.1 - 17/30\n",
      "train_loss: 0.03540 | train_acc: 100.000 | train_precision: 1.000 | train_recall: 0.005 | train_auc: 1.000\n",
      "val_loss: 0.05260 | val_acc: 100.000 | val_precision: 1.000 | val_recall: 1.000 | val_auc: 1.000\n",
      "Patience increased to 1\n",
      "---------------------------------------------\n",
      "NERVE] - 5.1 - 18/30\n",
      "train_loss: 0.03132 | train_acc: 100.000 | train_precision: 1.000 | train_recall: 0.004 | train_auc: 1.000\n",
      "val_loss: 0.04856 | val_acc: 100.000 | val_precision: 1.000 | val_recall: 1.000 | val_auc: 1.000\n",
      "---------------------------------------------\n",
      "NERVE] - 5.1 - 19/30\n",
      "train_loss: 0.02860 | train_acc: 100.000 | train_precision: 1.000 | train_recall: 0.003 | train_auc: 1.000\n",
      "val_loss: 0.05008 | val_acc: 100.000 | val_precision: 1.000 | val_recall: 1.000 | val_auc: 1.000\n",
      "Patience increased to 1\n",
      "---------------------------------------------\n",
      "NERVE] - 5.1 - 20/30\n",
      "train_loss: 0.02723 | train_acc: 100.000 | train_precision: 1.000 | train_recall: 0.003 | train_auc: 1.000\n",
      "val_loss: 0.04262 | val_acc: 100.000 | val_precision: 1.000 | val_recall: 1.000 | val_auc: 1.000\n",
      "---------------------------------------------\n",
      "NERVE] - 5.1 - 21/30\n",
      "train_loss: 0.02531 | train_acc: 100.000 | train_precision: 1.000 | train_recall: 0.003 | train_auc: 1.000\n",
      "val_loss: 0.04301 | val_acc: 100.000 | val_precision: 1.000 | val_recall: 1.000 | val_auc: 1.000\n",
      "Patience increased to 1\n",
      "---------------------------------------------\n",
      "NERVE] - 5.1 - 22/30\n",
      "train_loss: 0.02397 | train_acc: 100.000 | train_precision: 1.000 | train_recall: 0.002 | train_auc: 1.000\n",
      "val_loss: 0.03946 | val_acc: 100.000 | val_precision: 1.000 | val_recall: 1.000 | val_auc: 1.000\n",
      "---------------------------------------------\n",
      "NERVE] - 5.1 - 23/30\n",
      "train_loss: 0.02256 | train_acc: 100.000 | train_precision: 1.000 | train_recall: 0.002 | train_auc: 1.000\n",
      "val_loss: 0.03993 | val_acc: 100.000 | val_precision: 1.000 | val_recall: 1.000 | val_auc: 1.000\n",
      "Patience increased to 1\n",
      "---------------------------------------------\n",
      "NERVE] - 5.1 - 24/30\n",
      "train_loss: 0.02151 | train_acc: 100.000 | train_precision: 1.000 | train_recall: 0.002 | train_auc: 1.000\n",
      "val_loss: 0.03646 | val_acc: 100.000 | val_precision: 1.000 | val_recall: 1.000 | val_auc: 1.000\n",
      "---------------------------------------------\n",
      "NERVE] - 5.1 - 25/30\n",
      "train_loss: 0.01979 | train_acc: 100.000 | train_precision: 1.000 | train_recall: 0.001 | train_auc: 1.000\n",
      "val_loss: 0.03647 | val_acc: 100.000 | val_precision: 1.000 | val_recall: 1.000 | val_auc: 1.000\n",
      "Patience increased to 1\n",
      "---------------------------------------------\n",
      "NERVE] - 5.1 - 26/30\n",
      "train_loss: 0.01910 | train_acc: 100.000 | train_precision: 1.000 | train_recall: 0.001 | train_auc: 1.000\n",
      "val_loss: 0.03545 | val_acc: 100.000 | val_precision: 1.000 | val_recall: 1.000 | val_auc: 1.000\n",
      "---------------------------------------------\n",
      "NERVE] - 5.1 - 27/30\n",
      "train_loss: 0.01750 | train_acc: 100.000 | train_precision: 1.000 | train_recall: 0.001 | train_auc: 1.000\n",
      "val_loss: 0.03390 | val_acc: 100.000 | val_precision: 1.000 | val_recall: 1.000 | val_auc: 1.000\n",
      "---------------------------------------------\n",
      "NERVE] - 5.1 - 28/30\n",
      "train_loss: 0.01643 | train_acc: 100.000 | train_precision: 1.000 | train_recall: 0.001 | train_auc: 1.000\n",
      "val_loss: 0.03048 | val_acc: 100.000 | val_precision: 1.000 | val_recall: 1.000 | val_auc: 1.000\n",
      "---------------------------------------------\n",
      "NERVE] - 5.1 - 29/30\n",
      "train_loss: 0.01554 | train_acc: 100.000 | train_precision: 1.000 | train_recall: 0.001 | train_auc: 1.000\n",
      "val_loss: 0.03357 | val_acc: 100.000 | val_precision: 1.000 | val_recall: 1.000 | val_auc: 1.000\n",
      "Patience increased to 1\n",
      "---------------------------------------------\n",
      "NERVE] - 5.1 - 30/30\n",
      "train_loss: 0.01583 | train_acc: 100.000 | train_precision: 1.000 | train_recall: 0.001 | train_auc: 1.000\n",
      "val_loss: 0.03210 | val_acc: 100.000 | val_precision: 1.000 | val_recall: 1.000 | val_auc: 1.000\n",
      "---------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Encode the labels\n",
    "new_train_labels_int = np.array(\n",
    "    [get_int_label(label, group) for label in new_train_labels])\n",
    "val_labels_int = np.array(\n",
    "    [get_int_label(label, group) for label in val_labels])\n",
    "\n",
    "# Create datasets and Dataloaders\n",
    "train_dataset = CNS(new_train_features, new_train_labels_int, mode='train')\n",
    "val_dataset = CNS(val_features, val_labels_int, mode='val')\n",
    "train_loader = DataLoader(\n",
    "    train_dataset, batch_size=clf_cfg['mlp_train_batch_size'], shuffle=True)\n",
    "val_loader = DataLoader(\n",
    "    val_dataset, batch_size=clf_cfg['mlp_val_batch_size'], shuffle=False)\n",
    "\n",
    "# Init model object\n",
    "in_features = clf_cfg['n_features']\n",
    "model = DNAMLP(in_features, clf_cfg['n_classes'])\n",
    "if clf_cfg['MLP_FIRST_TIME'] == False:\n",
    "    # Load model based on fold\n",
    "    BEST_STATE_PATH = os.path.join(\n",
    "        clf_cfg['MLP_BEST_STATES_DIR'], group, f'{fold}.pth')\n",
    "    model.load_state_dict(torch.load(BEST_STATE_PATH))\n",
    "\n",
    "# Define training and validating hyperparams\n",
    "criterion = CrossEntropyLoss(weight=None)\n",
    "optimizer = Adam(model.parameters(\n",
    "), lr=clf_cfg['mlp_lr'], weight_decay=clf_cfg['mlp_weight_decay'])\n",
    "print(f'Running in {save} mode')\n",
    "best_accs = run(\n",
    "    group, fold, train_loader, val_loader, model, criterion, optimizer, clf_cfg, save)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100.0\n"
     ]
    }
   ],
   "source": [
    "print(best_accs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.5 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "19d1d53a962d236aa061289c2ac16dc8e6d9648c89fe79f459ae9a3493bc67b4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
