{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../../../src')\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import pyreadr\n",
    "import config\n",
    "import Dataset\n",
    "import time\n",
    "import train_MLP\n",
    "import argparse\n",
    "import utils\n",
    "import joblib\n",
    "\n",
    "from Dataset import CNS\n",
    "from torch.utils.data import DataLoader\n",
    "# from Model import DNAMLP\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from torch.optim import Adam\n",
    "from utils import make_ndarray_from_csv, get_int_label, brier_score_tensor\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score \n",
    "from torch.nn.functional import softmax, one_hot\n",
    "from sklearn.model_selection import train_test_split\n",
    "from imblearn.over_sampling import SMOTE, RandomOverSampler\n",
    "from sklearn.utils.class_weight import compute_class_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class DNAMLP(nn.Module):\n",
    "    def __init__ (self, in_features, n_classes):\n",
    "        super(DNAMLP, self).__init__()\n",
    "        self.in_features = in_features\n",
    "        self.n_classes = n_classes\n",
    "        self.densenet = nn.Sequential(\n",
    "            nn.Linear(self.in_features, 8),\n",
    "            nn.Dropout(p = 0.8),\n",
    "            nn.Linear(8, self.n_classes)\n",
    "        )\n",
    "        \n",
    "    def forward (self, x):\n",
    "        x = self.densenet(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(epoch, model, train_loader, criterion, optimizer, device):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    total_loss = 0\n",
    "    total_bs = 0\n",
    "    model.to(device)\n",
    "    model.train()\n",
    "    \n",
    "    # For loop through all batches\n",
    "    all_labels = []\n",
    "    all_logits = []\n",
    "    for features, labels in train_loader:\n",
    "        # Move tensors to device\n",
    "        features = features.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        # Zero out gradient\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        logits = model(features)\n",
    "        loss = criterion(logits, labels)\n",
    "        \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Evaluation\n",
    "        total_loss += loss.item()\n",
    "        _, predicted = logits.max(1)\n",
    "        correct += predicted.eq(labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "        \n",
    "        # batch BS\n",
    "        batch_bs = brier_score_tensor(logits, labels)\n",
    "        total_bs += batch_bs\n",
    "        \n",
    "        # save logits and labels to calculate AUC\n",
    "        for logit, label in zip(logits, labels):\n",
    "            all_labels.append(label.item())\n",
    "            all_logits.append(np.array(logit.detach().cpu().numpy()))\n",
    "        \n",
    "    # epoch's avrage LL\n",
    "    train_loss = total_loss / len(train_loader)\n",
    "    # epoch's average acc & ME\n",
    "    train_acc = (correct / total) * 100.\n",
    "    train_me = 100 - train_acc\n",
    "    # epoch's average BS\n",
    "    train_bs = total_bs/len(train_loader)\n",
    "    all_probs = softmax(torch.Tensor(np.array(all_logits)), dim = 1)\n",
    "    \n",
    "    all_preds = [np.argmax(prob) for prob in all_probs]\n",
    "    # epoch's AUC\n",
    "    train_auc = roc_auc_score(all_labels, all_preds)\n",
    "    train_precision = precision_score(all_labels, all_preds)\n",
    "    train_recall = recall_score(all_labels, all_preds)\n",
    "    \n",
    "    return train_loss, train_acc, train_me, train_bs, train_auc, train_precision, train_recall\n",
    "\n",
    "def val_epoch(epoch, model, val_loader, criterion, device):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    total_loss = 0\n",
    "    total_bs = 0\n",
    "    model.to(device)\n",
    "    # For loop through all batches\n",
    "    with torch.no_grad():\n",
    "        # For loop through all batches\n",
    "        all_labels = []\n",
    "        all_logits = []\n",
    "        for features, labels in val_loader:\n",
    "            # Move tensors to device\n",
    "            features, labels = features.to(device), labels.to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            logits = model(features)\n",
    "            \n",
    "            # Evaluation and batch loss\n",
    "            loss = criterion(logits, labels)\n",
    "            total_loss += loss.item()\n",
    "            _, predicted = logits.max(1)\n",
    "            correct += predicted.eq(labels).sum().item()\n",
    "            total  += labels.size(0)\n",
    "            \n",
    "            # batch BS\n",
    "            batch_bs = brier_score_tensor(logits, labels)\n",
    "            total_bs += batch_bs\n",
    "            \n",
    "            # save logits and labels to calculate AUC\n",
    "            for logit, label in zip(logits,labels):\n",
    "                all_labels.append(label.item())\n",
    "                all_logits.append(np.array(logit.detach().cpu().numpy()))\n",
    "        \n",
    "        # epoch's average LL\n",
    "        val_loss = total_loss / len(val_loader)\n",
    "        # epoch's average acc & ME\n",
    "        val_acc = (correct / total) * 100\n",
    "        val_me = (100 - val_acc)\n",
    "        # epoch's average BS\n",
    "        val_bs = total_bs/len(val_loader)\n",
    "\n",
    "        all_probs = softmax(torch.Tensor(np.array(all_logits)), dim = 1)\n",
    "        \n",
    "        all_preds = [np.argmax(prob) for prob in all_probs]\n",
    "        \n",
    "        # epoch's AUC \n",
    "        val_auc = roc_auc_score(all_labels, all_preds)\n",
    "        val_precision = precision_score(all_labels, all_preds)\n",
    "        val_recall = recall_score(all_labels, all_preds)\n",
    "         \n",
    "    return val_loss, val_acc, val_me, val_bs, val_auc, val_precision, val_recall\n",
    "\n",
    "def run(class_name, fold, train_loader, val_loader, model, criterion, optimizer, config, save):\n",
    "    history = {'val_accs': [], 'val_losses': [], 'val_precisions': [], 'val_recalls': [], 'val_aucs': []}\n",
    "    \n",
    "    model.to(config['device'])\n",
    "    n_epochs = config['mlp_n_epochs']\n",
    "    BEST_STATES_DIR= config['MLP_BEST_STATES_DIR']\n",
    "    BEST_STATE_PATH = os.path.join(BEST_STATES_DIR, class_name, f'{fold}.pth')\n",
    "    diff_threshold = config['mlp_diff_threshold']\n",
    "    max_patience = config['mlp_max_patience']\n",
    "    patience = 0\n",
    "    \n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "        # print(f'Epoch {epoch}/{n_epochs} of fold {fold}')\n",
    "        \n",
    "        train_loss, train_acc, train_me, train_bs, train_auc, train_precision, train_recall = train_epoch(epoch, model, train_loader, criterion, optimizer, config['device'])\n",
    "        val_loss, val_acc, val_me, val_bs, val_auc, val_precision, val_recall = val_epoch(epoch, model, val_loader, criterion, config['device'])\n",
    "        \n",
    "        # history['val_accs'].append(val_acc)\n",
    "        history['val_losses'].append(val_loss)\n",
    "        # history['val_precisions'].append(val_precision)\n",
    "        # history['val_recalls'].append(val_recall)\n",
    "        # history['val_aucs'].append(val_auc)\n",
    "        \n",
    "        # print(f'[{class_name.upper()}] - {fold} - {epoch}/{n_epochs}')\n",
    "        # print('train_loss: %.5f | train_acc: %.3f | train_precision: %.3f | train_recall: %.3f | train_auc: %.3f' % (train_loss, train_acc, train_precision, train_bs, train_auc))\n",
    "        # print('val_loss: %.5f | val_acc: %.3f | val_precision: %.3f | val_recall: %.3f | val_auc: %.3f' % (val_loss, val_acc, val_precision, val_recall, val_auc))\n",
    "        \n",
    "        if val_loss == min(history['val_losses']):\n",
    "            # get best epoch's resutls\n",
    "            best_epoch_results = {'train_loss: ': train_loss, 'train_accs': train_acc, 'train_me': train_me, 'train_bs': train_bs, 'train_precision': train_precision, 'train_recall': train_recall, 'train_auc': train_auc, 'val_loss: ': val_loss, 'val_accs': val_acc, 'val_me': val_me, 'val_bs': val_bs, 'val_precision': val_precision, 'val_recall': val_recall, 'val_auc': val_auc}\n",
    "            if save.lower() == 'save':\n",
    "                print('Lowest validation loss => saving model weights...')\n",
    "                torch.save(model.state_dict(), BEST_STATE_PATH)\n",
    "        if len(history['val_losses']) > 1:\n",
    "            if abs(history['val_losses'][-2] - val_loss) < diff_threshold or history['val_losses'][-2] < val_loss:\n",
    "                patience = patience + 1\n",
    "                print(f'Patience increased to {patience}')\n",
    "                if patience == max_patience:\n",
    "                    print('Early stopping.')\n",
    "                    break\n",
    "            else:\n",
    "                patience = 0\n",
    "        # print('---------------------------------------------')\n",
    "    return best_epoch_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running mlp classifiers\n",
      "root: /media/data/hungnt/work/SourceCode/Brain_DNA/src/..\n",
      "device: cuda\n",
      "save mode: no_save\n",
      "Melanocytic - 4.1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((119, 10000), (119,))"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Running MLP classifiers\")\n",
    "clf_cfg = config.classifier_config\n",
    "\n",
    "print(f'root: {config.root_dir}')\n",
    "print(f\"device: {clf_cfg['device']}\")\n",
    "print(f'save mode: {save}')\n",
    "\n",
    "folds = utils.inner_folds\n",
    "groups = utils.positive_groups\n",
    "\n",
    "fold = folds[np.random.randint(len(folds))]\n",
    "group = groups[np.random.randint(len(groups))]\n",
    "print(f'{group} - {fold}')\n",
    "\n",
    "# Read from csv to dataframe\n",
    "features, labels = make_ndarray_from_csv(group, fold, mode = 'all')\n",
    "features.shape, labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((23, 10000), (96, 10000))"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_features, val_features, train_labels, val_labels = train_test_split(features, labels, test_size=0.8, random_state=42)\n",
    "train_features.shape, val_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Control        18\n",
       " Melanocytic     5\n",
       " dtype: int64,\n",
       " 5)"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "value_counts = pd.Series(train_labels).value_counts()\n",
    "value_counts, min(value_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "save = 'no_save'\n",
    "use_SMOTE_values = [True, False]\n",
    "use_weights_values = [True, False]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SMOTE: True\n",
      "WEIGHTS: True\n",
      "['Control' 'Melanocytic']\n",
      "Control        18\n",
      "Melanocytic     5\n",
      "dtype: int64\n",
      "[0.61340206 2.70454545]\n",
      "(36, 10000)\n",
      "Running in no_save mode\n",
      "Patience increased to 1\n",
      "Patience increased to 1\n",
      "Patience increased to 2\n",
      "Patience increased to 3\n",
      "Patience increased to 1\n",
      "Patience increased to 2\n",
      "Patience increased to 1\n",
      "Patience increased to 2\n",
      "Patience increased to 1\n",
      "Patience increased to 2\n",
      "Patience increased to 1\n",
      "Patience increased to 2\n",
      "Patience increased to 1\n",
      ">>>>>>>>>>>>>>>>>>>>> BEST_EPOCH: {'train_loss: ': 0.2819985717535019, 'train_accs': 88.88888888888889, 'train_me': 11.111111111111114, 'train_bs': 0.17374497577692694, 'train_auc': 0.8888888888888888, 'train_precision': 0.8181818181818182, 'train_recall': 1.0, 'val_loss: ': 0.17550038049618402, 'val_accs': 100.0, 'val_me': 0.0, 'val_bs': 0.08945348245781842, 'val_auc': 1.0, 'val_precision': 1.0, 'val_recall': 1.0}\n",
      "SMOTE: True\n",
      "WEIGHTS: False\n",
      "(36, 10000)\n",
      "Running in no_save mode\n",
      "Patience increased to 1\n",
      "Patience increased to 2\n",
      "Patience increased to 1\n",
      "Patience increased to 1\n",
      "Patience increased to 1\n",
      "Patience increased to 2\n",
      "Patience increased to 1\n",
      "Patience increased to 2\n",
      "Patience increased to 1\n",
      "Patience increased to 1\n",
      ">>>>>>>>>>>>>>>>>>>>> BEST_EPOCH: {'train_loss: ': 0.2523092731833458, 'train_accs': 88.88888888888889, 'train_me': 11.111111111111114, 'train_bs': 0.14668625883150613, 'train_auc': 0.8888888888888888, 'train_precision': 1.0, 'train_recall': 0.7777777777777778, 'val_loss: ': 0.23568622022867203, 'val_accs': 86.45833333333334, 'val_me': 13.541666666666657, 'val_bs': 0.1503826009094745, 'val_auc': 0.9177215189873418, 'val_precision': 1.0, 'val_recall': 0.8354430379746836}\n",
      "SMOTE: False\n",
      "WEIGHTS: True\n",
      "['Control' 'Melanocytic']\n",
      "Control        18\n",
      "Melanocytic     5\n",
      "dtype: int64\n",
      "[0.61340206 2.70454545]\n",
      "(23, 10000)\n",
      "Running in no_save mode\n",
      "Patience increased to 1\n",
      "Patience increased to 2\n",
      "Patience increased to 1\n",
      "Patience increased to 1\n",
      "Patience increased to 2\n",
      "Patience increased to 1\n",
      "Patience increased to 2\n",
      "Patience increased to 1\n",
      "Patience increased to 1\n",
      "Patience increased to 1\n",
      "Patience increased to 1\n",
      "Patience increased to 2\n",
      "Patience increased to 1\n",
      ">>>>>>>>>>>>>>>>>>>>> BEST_EPOCH: {'train_loss: ': 0.23088155190149942, 'train_accs': 86.95652173913044, 'train_me': 13.043478260869563, 'train_bs': 0.13703347367911403, 'train_auc': 0.7, 'train_precision': 0.8571428571428571, 'train_recall': 1.0, 'val_loss: ': 0.20506194482247034, 'val_accs': 96.875, 'val_me': 3.125, 'val_bs': 0.11113927117371657, 'val_auc': 0.9117647058823529, 'val_precision': 0.9634146341463414, 'val_recall': 1.0}\n",
      "SMOTE: False\n",
      "WEIGHTS: False\n",
      "(23, 10000)\n",
      "Running in no_save mode\n",
      "Patience increased to 1\n",
      "Patience increased to 1\n",
      "Patience increased to 1\n",
      "Patience increased to 1\n",
      "Patience increased to 1\n",
      "Patience increased to 1\n",
      "Patience increased to 1\n",
      "Patience increased to 1\n",
      "Patience increased to 1\n",
      "Patience increased to 2\n",
      ">>>>>>>>>>>>>>>>>>>>> BEST_EPOCH: {'train_loss: ': 0.1625747929016749, 'train_accs': 95.65217391304348, 'train_me': 4.347826086956516, 'train_bs': 0.08262819612901044, 'train_auc': 0.9722222222222222, 'train_precision': 1.0, 'train_recall': 0.9444444444444444, 'val_loss: ': 0.21185938517252603, 'val_accs': 89.58333333333334, 'val_me': 10.416666666666657, 'val_bs': 0.1277342666681352, 'val_auc': 0.8905435591958302, 'val_precision': 0.9726027397260274, 'val_recall': 0.8987341772151899}\n"
     ]
    }
   ],
   "source": [
    "for use_SMOTE in use_SMOTE_values:\n",
    "    for use_weights in use_weights_values:\n",
    "        print(f'SMOTE: {use_SMOTE}\\nWEIGHTS: {use_weights}')\n",
    "        if use_weights == True:\n",
    "            class_weights = compute_class_weight(class_weight='balanced', classes=np.unique(train_labels), y = train_labels )\n",
    "            print(f'{np.unique(labels)}\\n{value_counts}\\n{class_weights}')\n",
    "        new_train_features, new_train_labels = train_features, train_labels\n",
    "        if use_SMOTE == True:\n",
    "            smote = SMOTE(sampling_strategy = \"auto\", random_state = 42, k_neighbors = min(value_counts) - 1)\n",
    "            new_train_features, new_train_labels = smote.fit_resample(train_features, train_labels)\n",
    "        print(new_train_features.shape)\n",
    "\n",
    "        # Encode the labels\n",
    "        new_train_labels_int = np.array(\n",
    "            [get_int_label(label, group) for label in new_train_labels])\n",
    "        val_labels_int = np.array(\n",
    "            [get_int_label(label, group) for label in val_labels])\n",
    "\n",
    "        # Create datasets and Dataloaders\n",
    "        train_dataset = CNS(new_train_features, new_train_labels_int, mode='train')\n",
    "        val_dataset = CNS(val_features, val_labels_int, mode='val')\n",
    "        train_loader = DataLoader(\n",
    "            train_dataset, batch_size=clf_cfg['mlp_train_batch_size'], shuffle=True)\n",
    "        val_loader = DataLoader(\n",
    "            val_dataset, batch_size=clf_cfg['mlp_val_batch_size'], shuffle=False)\n",
    "\n",
    "        # Init model object\n",
    "        in_features = clf_cfg['n_features']\n",
    "        model = DNAMLP(in_features, clf_cfg['n_classes'])\n",
    "        if clf_cfg['MLP_FIRST_TIME'] == False:\n",
    "            # Load model based on fold\n",
    "            BEST_STATE_PATH = os.path.join(\n",
    "                clf_cfg['MLP_BEST_STATES_DIR'], group, f'{fold}.pth')\n",
    "            model.load_state_dict(torch.load(BEST_STATE_PATH))\n",
    "\n",
    "        # Define training and validating hyperparams\n",
    "        criterion = CrossEntropyLoss(weight=None)\n",
    "        optimizer = Adam(model.parameters(\n",
    "        ), lr=clf_cfg['mlp_lr'], weight_decay=clf_cfg['mlp_weight_decay'])\n",
    "        print(f'Running in {save} mode')\n",
    "        best_epoch_results = run(\n",
    "            group, fold, train_loader, val_loader, model, criterion, optimizer, clf_cfg, save)\n",
    "        print(f'>>>>>>>>>>>>>>>>>>>>> BEST_EPOCH: {best_epoch_results}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.5 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "19d1d53a962d236aa061289c2ac16dc8e6d9648c89fe79f459ae9a3493bc67b4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
