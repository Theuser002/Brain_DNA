{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%reload_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_8364/1680280312.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfunctional\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0msoftmax\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mone_hot\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mconfusion_matrix\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 25\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mimblearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mover_sampling\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mSMOTE\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mRandomOverSampler\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     26\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclass_weight\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcompute_class_weight\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\ProgramData\\Miniconda3\\lib\\site-packages\\imblearn\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     50\u001b[0m     \u001b[1;31m# process, as it may not be compiled yet\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 52\u001b[1;33m     \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcombine\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     53\u001b[0m     \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mensemble\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     54\u001b[0m     \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mexceptions\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\ProgramData\\Miniconda3\\lib\\site-packages\\imblearn\\combine\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \"\"\"\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0m_smote_enn\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mSMOTEENN\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0m_smote_tomek\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mSMOTETomek\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\ProgramData\\Miniconda3\\lib\\site-packages\\imblearn\\combine\\_smote_enn.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mover_sampling\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mSMOTE\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mover_sampling\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbase\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mBaseOverSampler\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munder_sampling\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mEditedNearestNeighbours\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcheck_target_type\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mSubstitution\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\ProgramData\\Miniconda3\\lib\\site-packages\\imblearn\\under_sampling\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0m_prototype_generation\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mClusterCentroids\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0m_prototype_selection\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mRandomUnderSampler\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0m_prototype_selection\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mTomekLinks\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0m_prototype_selection\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mNearMiss\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\ProgramData\\Miniconda3\\lib\\site-packages\\imblearn\\under_sampling\\_prototype_selection\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0m_edited_nearest_neighbours\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mRepeatedEditedNearestNeighbours\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0m_edited_nearest_neighbours\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mAllKNN\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0m_instance_hardness_threshold\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mInstanceHardnessThreshold\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m __all__ = [\n",
      "\u001b[1;32mc:\\ProgramData\\Miniconda3\\lib\\site-packages\\imblearn\\under_sampling\\_prototype_selection\\_instance_hardness_threshold.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbase\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mClassifierMixin\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensemble\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mRandomForestClassifier\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     15\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensemble\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_base\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0m_set_random_states\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel_selection\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mStratifiedKFold\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\ProgramData\\Miniconda3\\lib\\site-packages\\sklearn\\ensemble\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0m_stacking\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mStackingClassifier\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0m_stacking\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mStackingRegressor\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 22\u001b[1;33m from ._hist_gradient_boosting.gradient_boosting import (\n\u001b[0m\u001b[0;32m     23\u001b[0m     \u001b[0mHistGradientBoostingRegressor\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m     \u001b[0mHistGradientBoostingClassifier\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\ProgramData\\Miniconda3\\lib\\site-packages\\sklearn\\ensemble\\_hist_gradient_boosting\\gradient_boosting.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mbinning\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0m_BinMapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 36\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mgrower\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mTreeGrower\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     37\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     38\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\ProgramData\\Miniconda3\\lib\\site-packages\\sklearn\\ensemble\\_hist_gradient_boosting\\grower.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0msplitting\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mSplitter\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mhistogram\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mHistogramBuilder\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mpredictor\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mTreePredictor\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     17\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0msum_parallel\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mcommon\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mPREDICTOR_RECORD_DTYPE\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\ProgramData\\Miniconda3\\lib\\site-packages\\sklearn\\ensemble\\_hist_gradient_boosting\\predictor.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mcommon\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mY_DTYPE\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0m_predictor\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0m_predict_from_raw_data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0m_predictor\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0m_predict_from_binned_data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0m_predictor\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0m_compute_partial_dependence\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\ProgramData\\Miniconda3\\lib\\importlib\\_bootstrap.py\u001b[0m in \u001b[0;36mparent\u001b[1;34m(self)\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('../../../src/')\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import config\n",
    "import itertools\n",
    "import utils\n",
    "import pickle\n",
    "\n",
    "from utils import impf_make_ndarray_from_csv, get_int_label, brier_score_tensor\n",
    "from Dataset import CNS\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from torch.optim import Adam\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score \n",
    "from torch.nn.functional import softmax, one_hot\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from imblearn.over_sampling import SMOTE, RandomOverSampler\n",
    "from sklearn.utils.class_weight import compute_class_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Impf_DNAMLP(nn.Module):\n",
    "    def __init__ (self, in_features, n_classes):\n",
    "        super(Impf_DNAMLP, self).__init__()\n",
    "        self.in_features = in_features\n",
    "        \n",
    "        self.n_classes = n_classes\n",
    "        self.densenet = nn.Sequential(\n",
    "            nn.Linear(self.in_features, self.n_classes),\n",
    "        )\n",
    "        \n",
    "    def forward (self, x):\n",
    "        x = self.densenet(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def impf_train_epoch(epoch, model, train_loader, criterion, optimizer, device):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    total_loss = 0\n",
    "    total_bs = 0\n",
    "    model.to(device)\n",
    "    model.train()\n",
    "    \n",
    "    # For loop through all batches\n",
    "    all_labels = []\n",
    "    all_logits = []\n",
    "    for features, labels in train_loader:\n",
    "        # Move tensors to device\n",
    "        features = features.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        # Zero out gradient\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        logits = model(features)\n",
    "        loss = criterion(logits, labels)\n",
    "\n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Evaluation\n",
    "        total_loss += loss.item()\n",
    "        _, predicted = logits.max(1)\n",
    "        correct += predicted.eq(labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "        \n",
    "        # batch BS\n",
    "        batch_bs = brier_score_tensor(logits, labels)\n",
    "        total_bs += batch_bs\n",
    "        \n",
    "        # save logits and labels to calculate AUC\n",
    "        for logit, label in zip(logits, labels):\n",
    "            all_labels.append(label.item())\n",
    "            all_logits.append(np.array(logit.detach().cpu().numpy()))\n",
    "        \n",
    "    # epoch's avrage LL\n",
    "    train_loss = total_loss / len(train_loader)\n",
    "    # epoch's average acc & ME\n",
    "    train_acc = (correct / total) * 100.\n",
    "    train_me = 100 - train_acc\n",
    "    # epoch's average BS\n",
    "    train_bs = total_bs/len(train_loader)\n",
    "    # epoch's average AUC\n",
    "    # all_labels_one_hot = one_hot(torch.Tensor(np.array(all_labels)).long())\n",
    "    all_probs = softmax(torch.Tensor(np.array(all_logits)), dim = 1)\n",
    "    # train_auc = roc_auc_score(all_labels_one_hot, all_probs)\n",
    "    \n",
    "    all_preds = [np.argmax(prob) for prob in all_probs]\n",
    "    train_auc = roc_auc_score(all_labels, all_preds)\n",
    "    train_precision = precision_score(all_labels, all_preds)\n",
    "    train_recall = recall_score(all_labels, all_preds)\n",
    "    train_cfs = confusion_matrix(all_labels, all_preds)\n",
    "    \n",
    "    return train_loss, train_acc, train_me, train_bs, train_auc, train_precision, train_recall, train_cfs\n",
    "\n",
    "def impf_val_epoch(epoch, model, val_loader, criterion, device):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    total_loss = 0\n",
    "    total_bs = 0\n",
    "    model.to(device)\n",
    "    # For loop through all batches\n",
    "    with torch.no_grad():\n",
    "        # For loop through all batches\n",
    "        all_labels = []\n",
    "        all_logits = []\n",
    "        for features, labels in val_loader:\n",
    "            # Move tensors to device\n",
    "            features, labels = features.to(device), labels.to(device)\n",
    "            # Forward pass\n",
    "            logits = model(features)\n",
    "            \n",
    "            # Evaluation and batch loss\n",
    "            loss = criterion(logits, labels)\n",
    "            total_loss += loss.item()\n",
    "            _, predicted = logits.max(1)\n",
    "            correct += predicted.eq(labels).sum().item()\n",
    "            total  += labels.size(0)\n",
    "            \n",
    "            # batch BS\n",
    "            batch_bs = brier_score_tensor(logits, labels)\n",
    "            total_bs += batch_bs\n",
    "            \n",
    "            # save logits and labels to calculate AUC\n",
    "            for logit, label in zip(logits,labels):\n",
    "                all_labels.append(label.item())\n",
    "                all_logits.append(np.array(logit.detach().cpu().numpy()))\n",
    "        \n",
    "        # epoch's average LL\n",
    "        val_loss = total_loss / len(val_loader)\n",
    "        # epoch's average acc & ME\n",
    "        val_acc = (correct / total) * 100\n",
    "        val_me = (100 - val_acc)\n",
    "        # epoch's average BS\n",
    "        val_bs = total_bs/len(val_loader)\n",
    "        # epoch's AUC\n",
    "        # all_labels_one_hot = one_hot(torch.Tensor(np.array(all_labels)).long())\n",
    "        all_probs = softmax(torch.Tensor(np.array(all_logits)), dim = 1)\n",
    "        # val_auc = roc_auc_score(all_labels_one_hot, all_probs)\n",
    "        \n",
    "        all_preds = [np.argmax(prob) for prob in all_probs]\n",
    "        val_auc = roc_auc_score(all_labels, all_preds)\n",
    "        val_precision = precision_score(all_labels, all_preds)\n",
    "        val_recall = recall_score(all_labels, all_preds)\n",
    "        val_cfs = confusion_matrix(all_labels, all_preds)\n",
    "         \n",
    "    return val_loss, val_acc, val_me, val_bs, val_auc, val_precision, val_recall, val_cfs\n",
    "\n",
    "def impf_run(class_name, alg, fold, train_loader, val_loader, model, criterion, optimizer, config, save):\n",
    "    history = {'val_accs': [], 'val_losses': [], 'val_precisions': [], 'val_recalls': [], 'val_aucs': [], 'test_accs': [], 'test_losses': [], 'test_precisions': [], 'test_recalls': [], 'test_aucs': []}\n",
    "    \n",
    "    model.to(config['device'])\n",
    "    n_epochs = config['mlp_n_epochs']\n",
    "    BEST_STATES_DIR= config['MLP_BEST_STATES_DIR']\n",
    "    BEST_STATE_PATH = os.path.join(BEST_STATES_DIR, alg, class_name, f'{fold}.pth')\n",
    "    diff_threshold = config['mlp_diff_threshold']\n",
    "    max_patience = config['mlp_max_patience']\n",
    "    patience = 0\n",
    "    \n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "        # print(f'Epoch {epoch}/{n_epochs} of fold {fold} of group {class_name}')\n",
    "        \n",
    "        train_loss, train_acc, train_me, train_bs, train_auc, train_precision, train_recall, train_cfs = impf_train_epoch(epoch, model, train_loader, criterion, optimizer, config['device'])\n",
    "        val_loss, val_acc, val_me, val_bs, val_auc, val_precision, val_recall, val_cfs = impf_val_epoch(epoch, model, val_loader, criterion, config['device'])\n",
    "        \n",
    "        # history['val_accs'].append(val_acc)\n",
    "        history['val_losses'].append(val_loss)\n",
    "        # history['val_precisions'].append(val_precision)\n",
    "        # history['val_recalls'].append(val_recall)\n",
    "        # history['val_aucs'].append(val_auc)\n",
    "        \n",
    "        # print(f'[{class_name.upper()}] - {fold} - {epoch}/{n_epochs}')\n",
    "        # print('train_loss: %.5f | train_acc: %.3f | train_precision: %.3f | train_recall: %.3f | train_auc: %.3f' % (train_loss, train_acc, train_precision, train_bs, train_auc))\n",
    "        # print('val_loss: %.5f | val_acc: %.3f | val_precision: %.3f | val_recall: %.3f | val_auc: %.3f' % (val_loss, val_acc, val_precision, val_recall, val_auc))\n",
    "        \n",
    "        if val_loss == min(history['val_losses']):\n",
    "            # get best epoch's resutls\n",
    "            best_epoch_results = {'train_loss: ': train_loss, 'train_accs': train_acc, 'train_me': train_me, 'train_bs': train_bs, 'train_precision': train_precision, 'train_recall': train_recall, 'train_auc': train_auc, 'train_cfs': train_cfs, 'val_loss: ': val_loss, 'val_accs': val_acc, 'val_me': val_me, 'val_bs': val_bs, 'val_precision': val_precision, 'val_recall': val_recall, 'val_auc': val_auc, 'val_cfs': val_cfs}\n",
    "            if save.lower() == 'save':\n",
    "                print('Lowest validation loss => saving model weights...')\n",
    "                torch.save(model.state_dict(), BEST_STATE_PATH)\n",
    "        if len(history['val_losses']) > 1:\n",
    "            patience = patience + 1\n",
    "            # print(f'Patience increased to {patience}')\n",
    "            if patience == max_patience:\n",
    "                # print('Early stopping.')\n",
    "                break\n",
    "            else:\n",
    "                patience = 0\n",
    "        # print('---------------------------------------------')\n",
    "    return best_epoch_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running impf_MLP classifier\n",
      "root: D:\\#Study\\Thesis\\Code\\Brain_DNA\\src\\..\n",
      "device: cpu\n"
     ]
    }
   ],
   "source": [
    "print('Running impf_MLP classifier')\n",
    "clf_cfg = config.classifier_config\n",
    "impf_cfg = config.impf_config\n",
    "device = clf_cfg['device']\n",
    "\n",
    "print(f'root: {config.root_dir}')\n",
    "print(f\"device: {device}\")\n",
    "\n",
    "groups = utils.positive_groups\n",
    "folds = utils.outer_folds\n",
    "\n",
    "fold = folds[np.random.randint(len(folds))]\n",
    "group = groups[np.random.randint(len(groups))]\n",
    "group = 'Embryonal'\n",
    "fold = '1.1'\n",
    "outer_fold = f'{fold.split(\".\")[0]}.0'\n",
    "alg = \"RF\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[MLP] | Embryonal - 1.1 - RF (2152)\n"
     ]
    }
   ],
   "source": [
    "with open(os.path.join(impf_cfg['IMPORTANT_FEATURES_DIR'], alg, group, f'{outer_fold}_combined.pkl'), 'rb') as file:\n",
    "    impf = pickle.load(file)    \n",
    "print(f'[MLP] | {group} - {fold} - {alg} ({len(impf)})')                     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((114, 2152), (459, 2152))"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features, labels = impf_make_ndarray_from_csv(group, fold, impf, mode = 'all')\n",
    "train_features, val_features, train_labels, val_labels = train_test_split(features, labels, test_size=0.8, random_state=42)\n",
    "train_features.shape, val_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Embryonal    96\n",
       " Control      18\n",
       " dtype: int64,\n",
       " 18)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "value_counts = pd.Series(train_labels).value_counts()\n",
    "value_counts, min(value_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save = 'no_save'\n",
    "use_SMOTE_values = [True, False]\n",
    "use_weights_values = [True, False]\n",
    "selected_metrics = ['train_precision', 'train_recall', 'train_auc', 'train_cfs', 'val_precision', 'val_recall', 'val_auc', 'val_cfs']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embryonal    96\n",
      "Control      18\n",
      "dtype: int64\n",
      "Running in no_save mode\n",
      "--------\n",
      "-> SMOTE: True - WEIGHTS: True | Best epoch: \n",
      "\ttrain_precision: 1.0\n",
      "\ttrain_recall: 1.0\n",
      "\ttrain_auc: 1.0\n",
      "\ttrain_cfs:\n",
      "[[96  0]\n",
      " [ 0 96]]\n",
      "\tval_precision: 1.0\n",
      "\tval_recall: 0.9459459459459459\n",
      "\tval_auc: 0.972972972972973\n",
      "\tval_cfs:\n",
      "[[385   0]\n",
      " [  4  70]]\n",
      "Running in no_save mode\n",
      "--------\n",
      "-> SMOTE: True - WEIGHTS: False | Best epoch: \n",
      "\ttrain_precision: 1.0\n",
      "\ttrain_recall: 1.0\n",
      "\ttrain_auc: 1.0\n",
      "\ttrain_cfs:\n",
      "[[96  0]\n",
      " [ 0 96]]\n",
      "\tval_precision: 0.9859154929577465\n",
      "\tval_recall: 0.9459459459459459\n",
      "\tval_auc: 0.9716742716742718\n",
      "\tval_cfs:\n",
      "[[384   1]\n",
      " [  4  70]]\n",
      "Running in no_save mode\n",
      "--------\n",
      "-> SMOTE: False - WEIGHTS: True | Best epoch: \n",
      "\ttrain_precision: 1.0\n",
      "\ttrain_recall: 1.0\n",
      "\ttrain_auc: 1.0\n",
      "\ttrain_cfs:\n",
      "[[96  0]\n",
      " [ 0 96]]\n",
      "\tval_precision: 1.0\n",
      "\tval_recall: 0.9459459459459459\n",
      "\tval_auc: 0.972972972972973\n",
      "\tval_cfs:\n",
      "[[385   0]\n",
      " [  4  70]]\n",
      "Running in no_save mode\n",
      "--------\n",
      "-> SMOTE: False - WEIGHTS: False | Best epoch: \n",
      "\ttrain_precision: 1.0\n",
      "\ttrain_recall: 1.0\n",
      "\ttrain_auc: 1.0\n",
      "\ttrain_cfs:\n",
      "[[96  0]\n",
      " [ 0 96]]\n",
      "\tval_precision: 0.9859154929577465\n",
      "\tval_recall: 0.9459459459459459\n",
      "\tval_auc: 0.9716742716742718\n",
      "\tval_cfs:\n",
      "[[384   1]\n",
      " [  4  70]]\n"
     ]
    }
   ],
   "source": [
    "print(f'{value_counts}')\n",
    "for use_SMOTE in use_SMOTE_values:\n",
    "    for use_weights in use_weights_values:\n",
    "        # print(f'SMOTE: {use_SMOTE}\\nWEIGHTS: {use_weights}')\n",
    "        if use_weights == True:\n",
    "            class_weights = torch.Tensor(compute_class_weight(class_weight='balanced', classes=np.unique(train_labels), y = train_labels ))\n",
    "        else:\n",
    "            class_weights = None\n",
    "        if use_SMOTE == True:\n",
    "            smote = SMOTE(sampling_strategy = \"auto\", random_state = 42, k_neighbors = max(1, min(value_counts) - 1)))\n",
    "            new_train_features, new_train_labels = smote.fit_resample(train_features, train_labels)\n",
    "        # print(new_train_features.shape)\n",
    "        # Encode the labels\n",
    "        new_train_labels_int = np.array([get_int_label(label, group) for label in new_train_labels])\n",
    "        val_labels_int = np.array([get_int_label(label, group) for label in val_labels])\n",
    "\n",
    "        # Create datasets and Dataloaders\n",
    "        train_dataset = CNS(new_train_features, new_train_labels_int, mode = 'train')\n",
    "        val_dataset = CNS(val_features, val_labels_int, mode = 'val')\n",
    "        train_loader = DataLoader(train_dataset, batch_size = impf_cfg['mlp_train_batch_size'], shuffle = True)\n",
    "        val_loader = DataLoader(val_dataset, batch_size = impf_cfg['mlp_val_batch_size'], shuffle = False)\n",
    "\n",
    "        # Init model object\n",
    "        in_features = len(impf)\n",
    "        model = Impf_DNAMLP(in_features, impf_cfg['n_classes'])\n",
    "            \n",
    "        # Define hyperparams\n",
    "        criterion = CrossEntropyLoss(weight=class_weights)\n",
    "        optimizer = Adam(model.parameters(), lr = impf_cfg['mlp_lr'], weight_decay = impf_cfg['mlp_weight_decay'])\n",
    "\n",
    "        print(f'Running in {save} mode')\n",
    "        best_epoch_results = impf_run(group, alg, fold, train_loader, val_loader, model, criterion, optimizer, impf_cfg, save) \n",
    "        print(f'--------\\n-> SMOTE: {use_SMOTE} - WEIGHTS: {use_weights} | Best epoch: ')\n",
    "        for key, value in best_epoch_results.items():\n",
    "            if key in selected_metrics:\n",
    "                if 'cfs' in key:\n",
    "                    print(f'\\t{key}:\\n{value}')\n",
    "                else:\n",
    "                    print(f'\\t{key}: {value}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.5 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "19d1d53a962d236aa061289c2ac16dc8e6d9648c89fe79f459ae9a3493bc67b4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
