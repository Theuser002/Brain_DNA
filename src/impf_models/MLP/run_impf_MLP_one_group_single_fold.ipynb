{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%reload_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../../../src/')\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import config\n",
    "import itertools\n",
    "import utils\n",
    "import pickle\n",
    "\n",
    "from utils import impf_make_ndarray_from_csv, get_int_label, brier_score_tensor\n",
    "from Dataset import CNS\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from torch.optim import Adam\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score \n",
    "from torch.nn.functional import softmax, one_hot\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from imblearn.over_sampling import SMOTE, RandomOverSampler\n",
    "from sklearn.utils.class_weight import compute_class_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Impf_DNAMLP(nn.Module):\n",
    "    def __init__ (self, in_features, n_classes):\n",
    "        super(Impf_DNAMLP, self).__init__()\n",
    "        self.in_features = in_features\n",
    "        \n",
    "        self.n_classes = n_classes\n",
    "        self.densenet = nn.Sequential(\n",
    "            nn.Linear(self.in_features, self.n_classes),\n",
    "        )\n",
    "        \n",
    "    def forward (self, x):\n",
    "        x = self.densenet(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def impf_train_epoch(epoch, model, train_loader, criterion, optimizer, device):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    total_loss = 0\n",
    "    total_bs = 0\n",
    "    model.to(device)\n",
    "    model.train()\n",
    "    \n",
    "    # For loop through all batches\n",
    "    all_labels = []\n",
    "    all_logits = []\n",
    "    for features, labels in train_loader:\n",
    "        # Move tensors to device\n",
    "        features = features.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        # Zero out gradient\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        logits = model(features)\n",
    "        loss = criterion(logits, labels)\n",
    "\n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Evaluation\n",
    "        total_loss += loss.item()\n",
    "        _, predicted = logits.max(1)\n",
    "        correct += predicted.eq(labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "        \n",
    "        # batch BS\n",
    "        batch_bs = brier_score_tensor(logits, labels)\n",
    "        total_bs += batch_bs\n",
    "        \n",
    "        # save logits and labels to calculate AUC\n",
    "        for logit, label in zip(logits, labels):\n",
    "            all_labels.append(label.item())\n",
    "            all_logits.append(np.array(logit.detach().cpu().numpy()))\n",
    "        \n",
    "    # epoch's avrage LL\n",
    "    train_loss = total_loss / len(train_loader)\n",
    "    # epoch's average acc & ME\n",
    "    train_acc = (correct / total) * 100.\n",
    "    train_me = 100 - train_acc\n",
    "    # epoch's average BS\n",
    "    train_bs = total_bs/len(train_loader)\n",
    "    # epoch's average AUC\n",
    "    # all_labels_one_hot = one_hot(torch.Tensor(np.array(all_labels)).long())\n",
    "    all_probs = softmax(torch.Tensor(np.array(all_logits)), dim = 1)\n",
    "    # train_auc = roc_auc_score(all_labels_one_hot, all_probs)\n",
    "    \n",
    "    all_preds = [np.argmax(prob) for prob in all_probs]\n",
    "    train_auc = roc_auc_score(all_labels, all_preds)\n",
    "    train_precision = precision_score(all_labels, all_preds)\n",
    "    train_recall = recall_score(all_labels, all_preds)\n",
    "    train_cfs = confusion_matrix(all_labels, all_preds)\n",
    "    \n",
    "    return train_loss, train_acc, train_me, train_bs, train_auc, train_precision, train_recall, train_cfs\n",
    "\n",
    "def impf_val_epoch(epoch, model, val_loader, criterion, device):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    total_loss = 0\n",
    "    total_bs = 0\n",
    "    model.to(device)\n",
    "    # For loop through all batches\n",
    "    with torch.no_grad():\n",
    "        # For loop through all batches\n",
    "        all_labels = []\n",
    "        all_logits = []\n",
    "        for features, labels in val_loader:\n",
    "            # Move tensors to device\n",
    "            features, labels = features.to(device), labels.to(device)\n",
    "            # Forward pass\n",
    "            logits = model(features)\n",
    "            \n",
    "            # Evaluation and batch loss\n",
    "            loss = criterion(logits, labels)\n",
    "            total_loss += loss.item()\n",
    "            _, predicted = logits.max(1)\n",
    "            correct += predicted.eq(labels).sum().item()\n",
    "            total  += labels.size(0)\n",
    "            \n",
    "            # batch BS\n",
    "            batch_bs = brier_score_tensor(logits, labels)\n",
    "            total_bs += batch_bs\n",
    "            \n",
    "            # save logits and labels to calculate AUC\n",
    "            for logit, label in zip(logits,labels):\n",
    "                all_labels.append(label.item())\n",
    "                all_logits.append(np.array(logit.detach().cpu().numpy()))\n",
    "        \n",
    "        # epoch's average LL\n",
    "        val_loss = total_loss / len(val_loader)\n",
    "        # epoch's average acc & ME\n",
    "        val_acc = (correct / total) * 100\n",
    "        val_me = (100 - val_acc)\n",
    "        # epoch's average BS\n",
    "        val_bs = total_bs/len(val_loader)\n",
    "        # epoch's AUC\n",
    "        # all_labels_one_hot = one_hot(torch.Tensor(np.array(all_labels)).long())\n",
    "        all_probs = softmax(torch.Tensor(np.array(all_logits)), dim = 1)\n",
    "        # val_auc = roc_auc_score(all_labels_one_hot, all_probs)\n",
    "        \n",
    "        all_preds = [np.argmax(prob) for prob in all_probs]\n",
    "        val_auc = roc_auc_score(all_labels, all_preds)\n",
    "        val_precision = precision_score(all_labels, all_preds)\n",
    "        val_recall = recall_score(all_labels, all_preds)\n",
    "        val_cfs = confusion_matrix(all_labels, all_preds)\n",
    "         \n",
    "    return val_loss, val_acc, val_me, val_bs, val_auc, val_precision, val_recall, val_cfs\n",
    "\n",
    "def impf_run(class_name, alg, fold, train_loader, val_loader, model, criterion, optimizer, config, save):\n",
    "    history = {'val_accs': [], 'val_losses': [], 'val_precisions': [], 'val_recalls': [], 'val_aucs': [], 'test_accs': [], 'test_losses': [], 'test_precisions': [], 'test_recalls': [], 'test_aucs': []}\n",
    "    \n",
    "    model.to(config['device'])\n",
    "    n_epochs = config['mlp_n_epochs']\n",
    "    BEST_STATES_DIR= config['MLP_BEST_STATES_DIR']\n",
    "    BEST_STATE_PATH = os.path.join(BEST_STATES_DIR, alg, class_name, f'{fold}.pth')\n",
    "    diff_threshold = config['mlp_diff_threshold']\n",
    "    max_patience = config['mlp_max_patience']\n",
    "    patience = 0\n",
    "    \n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "        # print(f'Epoch {epoch}/{n_epochs} of fold {fold} of group {class_name}')\n",
    "        \n",
    "        train_loss, train_acc, train_me, train_bs, train_auc, train_precision, train_recall, train_cfs = impf_train_epoch(epoch, model, train_loader, criterion, optimizer, config['device'])\n",
    "        val_loss, val_acc, val_me, val_bs, val_auc, val_precision, val_recall, val_cfs = impf_val_epoch(epoch, model, val_loader, criterion, config['device'])\n",
    "        \n",
    "        # history['val_accs'].append(val_acc)\n",
    "        history['val_losses'].append(val_loss)\n",
    "        # history['val_precisions'].append(val_precision)\n",
    "        # history['val_recalls'].append(val_recall)\n",
    "        # history['val_aucs'].append(val_auc)\n",
    "        \n",
    "        # print(f'[{class_name.upper()}] - {fold} - {epoch}/{n_epochs}')\n",
    "        # print('train_loss: %.5f | train_acc: %.3f | train_precision: %.3f | train_recall: %.3f | train_auc: %.3f' % (train_loss, train_acc, train_precision, train_bs, train_auc))\n",
    "        # print('val_loss: %.5f | val_acc: %.3f | val_precision: %.3f | val_recall: %.3f | val_auc: %.3f' % (val_loss, val_acc, val_precision, val_recall, val_auc))\n",
    "        \n",
    "        if val_loss == min(history['val_losses']):\n",
    "            # get best epoch's resutls\n",
    "            best_epoch_results = {'train_loss: ': train_loss, 'train_accs': train_acc, 'train_me': train_me, 'train_bs': train_bs, 'train_precision': train_precision, 'train_recall': train_recall, 'train_auc': train_auc, 'train_cfs': train_cfs, 'val_loss: ': val_loss, 'val_accs': val_acc, 'val_me': val_me, 'val_bs': val_bs, 'val_precision': val_precision, 'val_recall': val_recall, 'val_auc': val_auc, 'val_cfs': val_cfs}\n",
    "            if save.lower() == 'save':\n",
    "                print('Lowest validation loss => saving model weights...')\n",
    "                torch.save(model.state_dict(), BEST_STATE_PATH)\n",
    "        if len(history['val_losses']) > 1:\n",
    "            patience = patience + 1\n",
    "            # print(f'Patience increased to {patience}')\n",
    "            if patience == max_patience:\n",
    "                # print('Early stopping.')\n",
    "                break\n",
    "            else:\n",
    "                patience = 0\n",
    "        # print('---------------------------------------------')\n",
    "    return best_epoch_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running impf_MLP classifier\n",
      "root: /media/data/hungnt/work/SourceCode/Brain_DNA/src/..\n",
      "device: cuda\n"
     ]
    }
   ],
   "source": [
    "print('Running impf_MLP classifier')\n",
    "clf_cfg = config.classifier_config\n",
    "impf_cfg = config.impf_config\n",
    "\n",
    "print(f'root: {config.root_dir}')\n",
    "print(f\"device: {clf_cfg['device']}\")\n",
    "\n",
    "groups = utils.positive_groups\n",
    "folds = utils.outer_folds\n",
    "\n",
    "fold = folds[np.random.randint(len(folds))]\n",
    "group = groups[np.random.randint(len(groups))]\n",
    "outer_fold = f'{fold.split(\".\")[0]}.0'\n",
    "alg = \"RF\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[MLP] | Glio-neuronal - 4.0 - RF (1852)\n"
     ]
    }
   ],
   "source": [
    "with open(os.path.join(impf_cfg['IMPORTANT_FEATURES_DIR'], alg, group, f'{outer_fold}_combined.pkl'), 'rb') as file:\n",
    "    impf = pickle.load(file)    \n",
    "print(f'[MLP] | {group} - {fold} - {alg} ({len(impf)})')                     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((63, 1852), (254, 1852))"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features, labels = impf_make_ndarray_from_csv(group, fold, impf, mode = 'all')\n",
    "train_features, val_features, train_labels, val_labels = train_test_split(features, labels, test_size=0.8, random_state=42)\n",
    "train_features.shape, val_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Glio-neuronal    42\n",
       " Control          21\n",
       " dtype: int64,\n",
       " 21)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "value_counts = pd.Series(train_labels).value_counts()\n",
    "value_counts, min(value_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "save = 'no_save'\n",
    "use_SMOTE_values = [True, False]\n",
    "use_weights_values = [True, False]\n",
    "selected_metrics = ['train_precision', 'train_recall', 'train_auc', 'train_cfs', 'val_precision', 'val_recall', 'val_auc', 'val_cfs']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Glio-neuronal    42\n",
      "Control          21\n",
      "dtype: int64\n",
      "Running in no_save mode\n",
      "--------\n",
      "-> SMOTE: True - WEIGHTS: True | Best epoch: \n",
      "\ttrain_precision: 0.9333333333333333\n",
      "\ttrain_recall: 1.0\n",
      "\ttrain_auc: 0.9642857142857143\n",
      "\ttrain_cfs:\n",
      "[[39  3]\n",
      " [ 0 42]]\n",
      "\tval_precision: 0.7419354838709677\n",
      "\tval_recall: 0.9387755102040817\n",
      "\tval_auc: 0.8668236525379384\n",
      "\tval_cfs:\n",
      "[[124  32]\n",
      " [  6  92]]\n",
      "Running in no_save mode\n",
      "--------\n",
      "-> SMOTE: True - WEIGHTS: False | Best epoch: \n",
      "\ttrain_precision: 0.8936170212765957\n",
      "\ttrain_recall: 1.0\n",
      "\ttrain_auc: 0.9404761904761905\n",
      "\ttrain_cfs:\n",
      "[[37  5]\n",
      " [ 0 42]]\n",
      "\tval_precision: 0.7258064516129032\n",
      "\tval_recall: 0.9183673469387755\n",
      "\tval_auc: 0.8502093144950287\n",
      "\tval_cfs:\n",
      "[[122  34]\n",
      " [  8  90]]\n",
      "Running in no_save mode\n",
      "--------\n",
      "-> SMOTE: False - WEIGHTS: True | Best epoch: \n",
      "\ttrain_precision: 0.9333333333333333\n",
      "\ttrain_recall: 1.0\n",
      "\ttrain_auc: 0.9642857142857143\n",
      "\ttrain_cfs:\n",
      "[[39  3]\n",
      " [ 0 42]]\n",
      "\tval_precision: 0.7586206896551724\n",
      "\tval_recall: 0.8979591836734694\n",
      "\tval_auc: 0.859236002093145\n",
      "\tval_cfs:\n",
      "[[128  28]\n",
      " [ 10  88]]\n",
      "Running in no_save mode\n",
      "--------\n",
      "-> SMOTE: False - WEIGHTS: False | Best epoch: \n",
      "\ttrain_precision: 0.9333333333333333\n",
      "\ttrain_recall: 1.0\n",
      "\ttrain_auc: 0.9642857142857143\n",
      "\ttrain_cfs:\n",
      "[[39  3]\n",
      " [ 0 42]]\n",
      "\tval_precision: 0.75\n",
      "\tval_recall: 0.9489795918367347\n",
      "\tval_auc: 0.875130821559393\n",
      "\tval_cfs:\n",
      "[[125  31]\n",
      " [  5  93]]\n"
     ]
    }
   ],
   "source": [
    "print(f'{value_counts}')\n",
    "for use_SMOTE in use_SMOTE_values:\n",
    "    for use_weights in use_weights_values:\n",
    "        # print(f'SMOTE: {use_SMOTE}\\nWEIGHTS: {use_weights}')\n",
    "        if use_weights == True:\n",
    "            class_weights = compute_class_weight(class_weight='balanced', classes=np.unique(train_labels), y = train_labels )\n",
    "        if use_SMOTE == True:\n",
    "            smote = SMOTE(sampling_strategy = \"auto\", random_state = 42, k_neighbors = min(value_counts) - 1)\n",
    "            new_train_features, new_train_labels = smote.fit_resample(train_features, train_labels)\n",
    "        # print(new_train_features.shape)\n",
    "        # Encode the labels\n",
    "        new_train_labels_int = np.array([get_int_label(label, group) for label in new_train_labels])\n",
    "        val_labels_int = np.array([get_int_label(label, group) for label in val_labels])\n",
    "\n",
    "        # Create datasets and Dataloaders\n",
    "        train_dataset = CNS(new_train_features, new_train_labels_int, mode = 'train')\n",
    "        val_dataset = CNS(val_features, val_labels_int, mode = 'val')\n",
    "        train_loader = DataLoader(train_dataset, batch_size = impf_cfg['mlp_train_batch_size'], shuffle = True)\n",
    "        val_loader = DataLoader(val_dataset, batch_size = impf_cfg['mlp_val_batch_size'], shuffle = False)\n",
    "\n",
    "        # Init model object\n",
    "        in_features = len(impf)\n",
    "        model = Impf_DNAMLP(in_features, impf_cfg['n_classes'])\n",
    "            \n",
    "        # Define hyperparams\n",
    "        criterion = CrossEntropyLoss(weight=None)\n",
    "        optimizer = Adam(model.parameters(), lr = impf_cfg['mlp_lr'], weight_decay = impf_cfg['mlp_weight_decay'])\n",
    "\n",
    "        print(f'Running in {save} mode')\n",
    "        best_epoch_results = impf_run(group, alg, fold, train_loader, val_loader, model, criterion, optimizer, impf_cfg, save) \n",
    "        print(f'--------\\n-> SMOTE: {use_SMOTE} - WEIGHTS: {use_weights} | Best epoch: ')\n",
    "        for key, value in best_epoch_results.items():\n",
    "            if key in selected_metrics:\n",
    "                if 'cfs' in key:\n",
    "                    print(f'\\t{key}:\\n{value}')\n",
    "                else:\n",
    "                    print(f'\\t{key}: {value}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.5 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f1a95063673afb40ff742ea38932ca37761c5e5b95ffd28e8b29d2c50499cde7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
